# .env.sample - Sample environment configuration for medparswell
# Copy this file to `.env` and fill in actual values as needed.

# Path to the compiled llama-cli binary (from ik_llama.cpp fork)
LLAMA_CLI_PATH=/path/to/llama-cli

# Path to the GGUF model file to be used for inference
LLAMA_MODEL_PATH=/path/to/your-model.gguf

# Context size (number of tokens the model can see)
LLAMA_CONTEXT_SIZE=2048

# Number of layers to offload to the GPU (optimize based on your system)
LLAMA_GPU_LAYERS=2

# GPU index to run inference on (default 0)
LLAMA_MAIN_GPU=0

# NUMA setting for model execution (e.g., "isolate", "interleave")
LLAMA_NUMA=isolate

# Enable verbose output from llama-cli
LLAMA_VERBOSE=true

# Logging configuration
LLAMA_LOG_LEVEL=DEBUG                     # Log level: DEBUG, INFO, WARNING, ERROR
LLAMA_LOG_FILE=logs/medparswell.log       # Path to output log file

# Maximum time (in seconds) to allow for llama-cli execution
LLAMA_CLI_TIMEOUT=360

# Interfaces to generate routes for
ENABLED_INTERFACES=swagger_ui
# Endpoints to register from app/endpoints/*/schema/
ENABLED_ENDPOINTS=ik_llama.text_summarization