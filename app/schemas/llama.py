from pydantic import BaseModel, Field
from typing import Optional


class LlamaCLIRequest(BaseModel):
    prompt: str = Field(..., description="The prompt to provide to the model.")
    model_path: Optional[str] = Field(None, description="Path to the .gguf model file.")
    ctx_size: Optional[int] = Field(1024, description="Context size (number of tokens).")
    gpu_layers: Optional[int] = Field(2, description="Number of layers to offload to the GPU.")
    main_gpu: Optional[int] = Field(0, description="Index of the GPU to use.")
    numa: Optional[bool] = Field(True, description="Whether to enable NUMA optimization.")
    verbose: Optional[bool] = Field(False, description="Enable verbose output from llama-cli.")


class LlamaCLIResponse(BaseModel):
    output: str = Field(..., description="The raw response generated by the model.")
    tokens_generated: Optional[int] = Field(None, description="Estimated number of tokens returned.")
    execution_time_ms: Optional[int] = Field(None, description="Time taken to generate the response (ms).")